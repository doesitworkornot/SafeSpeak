{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87433,"databundleVersionId":10051766,"sourceType":"competition"},{"sourceId":176260,"sourceType":"modelInstanceVersion","modelInstanceId":150079,"modelId":172570}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fairseq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:07:53.467606Z","iopub.execute_input":"2024-11-24T05:07:53.468345Z","iopub.status.idle":"2024-11-24T05:08:44.027388Z","shell.execute_reply.started":"2024-11-24T05:07:53.468313Z","shell.execute_reply":"2024-11-24T05:08:44.026504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nfrom typing import Union\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nimport fairseq\nimport argparse\n\nclass SSLModel(nn.Module):\n    def __init__(self, device):\n        super(SSLModel, self).__init__()\n        task_arg = argparse.Namespace(task='audio_pretraining')\n        task = fairseq.tasks.setup_task(task_arg)\n        cp_path = '/kaggle/input/w2v2_scoof/pytorch/default/1/xlsr2_300m.pt'   # Path to pre-trained model \n        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path], task=task)\n        self.model = model[0].to(device)  # Move the model to the specified device only once\n        self.device = device\n        self.out_dim = 1024\n\n    def extract_feat(self, input_data):\n        # Ensure input is on the correct device\n        input_data = input_data.to(self.device)\n\n        # Adjust input shape to (batch, length) if necessary\n        input_tmp = input_data[:, :, 0] if input_data.ndim == 3 else input_data\n                \n        # Extract features [batch, length, dim]\n        emb = self.model(input_tmp, mask=False, features_only=True)['x']\n        return emb\n\n\nclass PSFAN_Backend(nn.Module):\n    def __init__(self, input_channels=128, num_classes=2):\n        super(PSFAN_Backend, self).__init__()\n        \n        # First convolutional block with dilation rate = 1\n        self.conv1 = nn.Conv1d(input_channels, 128, kernel_size=3, dilation=1, padding=1)\n        self.conv1x1_1 = nn.Conv1d(128, 128, kernel_size=1)\n        self.conv3x3_1 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n        self.conv1x1_2 = nn.Conv1d(128, 128, kernel_size=1)\n        self.attention1 = nn.Sigmoid()\n        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n        \n        # Second convolutional block with dilation rate = 2\n        self.conv2 = nn.Conv1d(128, 128, kernel_size=3, dilation=2, padding=2)\n        self.conv1x1_3 = nn.Conv1d(128, 128, kernel_size=1)\n        self.conv3x3_2 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n        self.conv1x1_4 = nn.Conv1d(128, 128, kernel_size=1)\n        self.attention2 = nn.Sigmoid()\n        self.pool2 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n\n        # Third convolutional block with dilation rate = 3\n        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, dilation=3, padding=3)\n        self.conv1x1_5 = nn.Conv1d(256, 256, kernel_size=1)\n        self.conv3x3_3 = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n        self.conv1x1_6 = nn.Conv1d(256, 256, kernel_size=1)\n        self.attention3 = nn.Sigmoid()\n        self.pool3 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n\n        # Fourth convolutional block with dilation rate = 4\n        self.conv4 = nn.Conv1d(256, 256, kernel_size=3, dilation=4, padding=4)\n        self.conv1x1_7 = nn.Conv1d(256, 256, kernel_size=1)\n        self.conv3x3_4 = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n        self.conv1x1_8 = nn.Conv1d(256, 256, kernel_size=1)\n        self.attention4 = nn.Sigmoid()\n        self.pool4 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n\n        # Global Average Pooling layer for each block output\n        self.gap1 = nn.AdaptiveAvgPool1d(1)\n        self.gap2 = nn.AdaptiveAvgPool1d(1)\n        self.gap3 = nn.AdaptiveAvgPool1d(1)\n        self.gap4 = nn.AdaptiveAvgPool1d(1)\n        \n        # Fully connected layers\n        self.fc_concat = nn.Linear(128 + 128 + 256 + 256, 16)  # Concatenated GAP output to dense layer\n        self.fc_out = nn.Linear(16, num_classes)  # Final output layer\n        \n        self.activation = nn.LeakyReLU(0.02)\n\n    def forward(self, x):\n        # First convolutional block with attention and pooling\n        x1 = self.conv1(x)\n        x1_attention = self.attention1(self.conv1x1_1(self.conv3x3_1(self.conv1x1_2(x1))))\n        x1 = x1_attention * x1\n        x1 = self.pool1(x1)\n        x1_gap = self.gap1(x1).squeeze(-1)  # Apply GAP and remove last dimension to (batch, channels)\n\n        # Second convolutional block with attention and pooling\n        x2 = self.conv2(x1)\n        x2_attention = self.attention2(self.conv1x1_3(self.conv3x3_2(self.conv1x1_4(x2))))\n        x2 = x2_attention * x2\n        x2 = self.pool2(x2)\n        x2_gap = self.gap2(x2).squeeze(-1)\n\n        # Third convolutional block with attention and pooling\n        x3 = self.conv3(x2)\n        x3_attention = self.attention3(self.conv1x1_5(self.conv3x3_3(self.conv1x1_6(x3))))\n        x3 = x3_attention * x3\n        x3 = self.pool3(x3)\n        x3_gap = self.gap3(x3).squeeze(-1)\n\n        # Fourth convolutional block with attention and pooling\n        x4 = self.conv4(x3)\n        x4_attention = self.attention4(self.conv1x1_7(self.conv3x3_4(self.conv1x1_8(x4))))\n        x4 = x4_attention * x4\n        x4 = self.pool4(x4)\n        x4_gap = self.gap4(x4).squeeze(-1)\n\n        # Concatenate the GAP outputs\n        x_concat = torch.cat([x1_gap, x2_gap, x3_gap, x4_gap], dim=1)  # Shape: (batch, 768)\n\n        # Fully connected layers for classification\n        x = self.activation(self.fc_concat(x_concat))  # Dense layer with 16 units\n        output = self.fc_out(x)  # Output layer with 2 units\n\n        return output\n\n\nclass Model(nn.Module):\n    def __init__(self, device):\n        super(Model, self).__init__()\n        self.device = device\n        \n        # wav2vec 2.0 front-end remains unchanged\n        self.ssl_model = SSLModel(self.device)\n        self.LL = nn.Linear(self.ssl_model.out_dim, 128).to(device)  # Reduces dimensionality to 128 for compatibility\n\n        # PSFAN backend with Conv1D for multi-scale feature extraction\n        self.backend = PSFAN_Backend(input_channels=128, num_classes=2).to(device)\n\n    def forward(self, x):\n        # Move input to the same device as model\n        x = x.to(self.device)\n\n        # wav2vec 2.0 feature extraction\n        x_ssl_feat = self.ssl_model.extract_feat(x)\n        x = self.LL(x_ssl_feat)  # Dimensionality reduction to 128 channels\n        x = x.transpose(1, 2)  # Reshape to (batch, features, timesteps) for Conv1D format\n        \n        # Backend processing for classification\n        output = self.backend(x)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:08:44.029787Z","iopub.execute_input":"2024-11-24T05:08:44.030171Z","iopub.status.idle":"2024-11-24T05:09:12.001495Z","shell.execute_reply.started":"2024-11-24T05:08:44.030127Z","shell.execute_reply":"2024-11-24T05:09:12.000805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = Model(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:09:12.002686Z","iopub.execute_input":"2024-11-24T05:09:12.003173Z","iopub.status.idle":"2024-11-24T05:09:47.932917Z","shell.execute_reply.started":"2024-11-24T05:09:12.003146Z","shell.execute_reply":"2024-11-24T05:09:47.931997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/input/w2v2_scoof/pytorch/default/1/w2v2_scoof.pth', map_location=device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:09:47.935107Z","iopub.execute_input":"2024-11-24T05:09:47.935993Z","iopub.status.idle":"2024-11-24T05:09:58.498530Z","shell.execute_reply.started":"2024-11-24T05:09:47.935951Z","shell.execute_reply":"2024-11-24T05:09:58.497678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndef get_data_for_dataset(path):\n    ids_list = []\n    label_list = []\n    with open(path, \"r\") as file:\n        for line in file:\n            line = line.split()\n            id, label = line[1], line[-1]\n            ids_list.append(id)\n            label = 1 if label == \"bonafide\" else 0\n            label_list.append(label)\n    return ids_list, label_list\n\ndef get_data_for_evaldataset(path):\n    ids_list = os.listdir(path)\n    return ids_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:09:58.499810Z","iopub.execute_input":"2024-11-24T05:09:58.500176Z","iopub.status.idle":"2024-11-24T05:09:58.505888Z","shell.execute_reply.started":"2024-11-24T05:09:58.500136Z","shell.execute_reply":"2024-11-24T05:09:58.504965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport soundfile as sf\n\ndef pad_random(x, max_len=64600):\n    x_len = x.shape[0]\n\n    if x_len > max_len:\n        stt = np.random.randint(x_len - max_len)\n        return x[stt:stt + max_len]\n\n    num_repeats = int(max_len / x_len) + 1\n    padded_x = np.tile(x, num_repeats)[:max_len]\n    return padded_x\n\n\ndef pad(x, max_len=64600):\n    x_len = x.shape[0]\n    if x_len >= max_len:\n        return x[:max_len]\n    # need to pad\n    num_repeats = int(max_len / x_len) + 1\n    padded_x = np.tile(x, (1, num_repeats))[:, :max_len][0]\n    return padded_x\n\nclass EvalDataset(Dataset):\n    def __init__(self, ids, dir_path, pad_fn=pad_random, cut=64600):\n        self.ids = ids\n        self.dir_path = dir_path\n        self.cut = cut\n        self.pad_fn = pad_fn\n\n    def __getitem__(self, index):\n        path_to_wav = f\"{self.dir_path}/{self.ids[index]}\"\n        audio, rate = sf.read(path_to_wav)\n        x_pad = self.pad_fn(audio, self.cut)\n        x_inp = Tensor(x_pad)\n        return x_inp, self.ids[index]\n\n    def __len__(self):\n        return len(self.ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T07:27:08.196151Z","iopub.execute_input":"2024-11-24T07:27:08.196975Z","iopub.status.idle":"2024-11-24T07:27:08.205629Z","shell.execute_reply.started":"2024-11-24T07:27:08.196938Z","shell.execute_reply":"2024-11-24T07:27:08.204686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = {\n  \"model\": \"ResCapsGuard\",\n  \"batch_size\": 8,\n  \"d_args\": {\n      \"nb_samp\": 64600,\n      \"first_conv\": 128,\n      \"filts\": [70, [1, 32], [32, 32], [32, 64], [64, 64]]\n  },\n  \"device\": \"cuda:0\",\n  \"num_class\": 2,\n  \"gpu_id\": 0,\n  \"dropout\": 0.05,\n  \"random_size\": 0.01,\n  \"num_iterations\": 2,\n  \"gamma\": 0.5,\n  \"step_size\": 10,\n  \"produced_file\": \"pruduced_file.txt\",\n  \"num_workers\": 6\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T07:27:11.325393Z","iopub.execute_input":"2024-11-24T07:27:11.325753Z","iopub.status.idle":"2024-11-24T07:27:11.330948Z","shell.execute_reply.started":"2024-11-24T07:27:11.325722Z","shell.execute_reply":"2024-11-24T07:27:11.330022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_dataloaders(datasets, config):\n    dataloaders = {}\n\n    if datasets.get(\"train\"):\n        train_loader = DataLoader(\n            datasets[\"train\"],\n            batch_size=config[\"batch_size\"],\n            shuffle=True,\n            num_workers=config[\"num_workers\"]\n        )\n        dataloaders[\"train\"] = train_loader\n    if datasets.get(\"dev\"):\n        dev_loader = DataLoader(\n            datasets[\"dev\"],\n            batch_size=config[\"batch_size\"],\n            shuffle=False,\n            num_workers=config[\"num_workers\"]\n        )\n        dataloaders[\"dev\"] = dev_loader\n\n    if datasets.get(\"eval\"):\n        eval_loader = DataLoader(\n            datasets[\"eval\"],\n            batch_size=config[\"batch_size\"],\n            shuffle=False,\n            num_workers=config[\"num_workers\"]\n        )\n        dataloaders[\"eval\"] = eval_loader\n\n    return dataloaders","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T07:27:12.587394Z","iopub.execute_input":"2024-11-24T07:27:12.588192Z","iopub.status.idle":"2024-11-24T07:27:12.593628Z","shell.execute_reply.started":"2024-11-24T07:27:12.588158Z","shell.execute_reply":"2024-11-24T07:27:12.592653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install progressbar\n!pip install soundfile","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T05:09:58.555184Z","iopub.execute_input":"2024-11-24T05:09:58.555592Z","iopub.status.idle":"2024-11-24T05:10:16.512513Z","shell.execute_reply.started":"2024-11-24T05:09:58.555554Z","shell.execute_reply":"2024-11-24T05:10:16.511443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport time\nimport torch.nn as nn\n\ndef progressbar(it, prefix=\"\", size=60, out=sys.stdout):  # Python3.6+\n    count = len(it)\n    start = time.time()\n\n    def show(j):\n        x = int(size * j / count)\n        remaining = ((time.time() - start) / j) * (count - j)\n        passing = time.time() - start\n        mins_pas, sec_pass = divmod(passing, 60)\n        time_pas = f\"{int(mins_pas):02}:{sec_pass:05.2f}\"\n\n        mins, sec = divmod(remaining, 60)\n        time_str = f\"{int(mins):02}:{sec:05.2f}\"\n\n        print(f\"{prefix}[{u'█' * x}{('.' * (size - x))}] {j}/{count} time {time_pas} / {time_str}\", end='\\r', file=out,\n              flush=True)\n\n    for i, item in enumerate(it):\n        yield item\n        show(i + 1)\n    print(\"\\n\", flush=True, file=out)\n\n@torch.inference_mode\ndef produce_submit_file(data_loader,\n                            model,\n                            device,\n                            save_path,\n                            random=False,\n                            dropout=0):\n    \"\"\"\n    Create file, that need to give in function calculcate_t-DCF_EER\n    args:\n        data_loader: loader, that gives batch to model\n        model: model, that calculate what we need\n        device: device for data, model\n        save_path: path where file shoud be saved\n    \"\"\"\n\n    # turning model into evaluation mode\n    model.eval()\n\n    # list of utterance id and list of score for appropiate uid\n    fname_list = []\n    score_list = []\n    # inference\n    for batch_x, utt_id in progressbar(data_loader, prefix='computing cm score'):\n        batch_x = batch_x.to(device)\n        with torch.no_grad():\n            # first is hidden layer, second is result\n            batch_out = model.forward(batch_x)\n            prob = nn.functional.softmax(batch_out, dim=1)\n            \n            # 1 - for bonafide speech class\n            batch_score = (prob[:, 1]).data.cpu().numpy().ravel()\n\n        # add outputs\n        fname_list.extend(utt_id)\n        score_list.extend(batch_score.tolist())\n    assert len(fname_list) == len(score_list)\n\n    return fname_list, score_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T07:27:16.419699Z","iopub.execute_input":"2024-11-24T07:27:16.420578Z","iopub.status.idle":"2024-11-24T07:27:16.429604Z","shell.execute_reply.started":"2024-11-24T07:27:16.420535Z","shell.execute_reply":"2024-11-24T07:27:16.428776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path_wav = '/kaggle/input/safe-speak-2024-audio-spoof-detection-hackathon/wavs'\nout_path = 'output_hz'\neval_ids = get_data_for_evaldataset(path_wav)\n\neval_dataset = EvalDataset(eval_ids, path_wav, pad)\neval_dataset = {\n    \"eval\": eval_dataset\n}\ndataloader = get_dataloaders(eval_dataset, config)\n\nfname_list, score_list = produce_submit_file(dataloader[\"eval\"], model, device, out_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T07:27:16.990393Z","iopub.execute_input":"2024-11-24T07:27:16.990780Z","iopub.status.idle":"2024-11-24T09:31:49.974014Z","shell.execute_reply.started":"2024-11-24T07:27:16.990749Z","shell.execute_reply":"2024-11-24T09:31:49.973073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nout_path = 'output_score.csv'\nwith open(out_path, \"w\") as fh:\n    for fn, sco in zip(fname_list, score_list):\n        if \".wav\" in fn:\n            fn = fn.replace(\".wav\", \"\")\n        fh.write(\"{} {}\\n\".format(fn, sco))\ndf = pd.read_csv(out_path, sep=\" \", names=[\"ID\", \"score\"])\ndf.to_csv(out_path, index=False)\nprint(\"Scores saved to {}\".format(out_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:49:03.299686Z","iopub.execute_input":"2024-11-24T09:49:03.300038Z","iopub.status.idle":"2024-11-24T09:49:03.803327Z","shell.execute_reply.started":"2024-11-24T09:49:03.300007Z","shell.execute_reply":"2024-11-24T09:49:03.802484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}